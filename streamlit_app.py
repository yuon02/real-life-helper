import streamlit as st
from utils.helper import get_topic_data
from googletrans import Translator
import requests
import json
from bs4 import BeautifulSoup
import urllib.parse
import re

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì²­ë…„ ì‹¤ìƒí™œ ì •ë³´ ê°€ì´ë“œ", layout="wide")

# âœ… ë¡œê³  ì´ë¯¸ì§€ ìƒë‹¨ì— í‘œì‹œ (GitHub ê²½ë¡œ)
st.image("https://raw.githubusercontent.com/yuon02/real-life-helper/main/logo.png", width=120)
st.markdown("<h1 style='color:#3F72AF;'>ì²­ë…„ ì‹¤ìƒí™œ ì •ë³´ ë„ìš°ë¯¸</h1>", unsafe_allow_html=True)
st.caption("ëª¨ë°”ì¼ì²˜ëŸ¼ í¸ë¦¬í•˜ê²Œ, í•„ìš”í•œ ìƒí™œ ì •ë³´ë¥¼ í•œëˆˆì— í™•ì¸í•˜ì„¸ìš”.")

# ì–¸ì–´ ì„¤ì •
lang = st.selectbox("ğŸŒ ì–¸ì–´ ì„ íƒ", ["í•œêµ­ì–´", "English"])
translator = Translator()
translate = lambda text: translator.translate(text, dest="en").text if lang == "English" else text

# ì£¼ì œ ì„ íƒ
main_topic = st.selectbox("ğŸ“Œ ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì•„ë¥´ë°”ì´íŠ¸", "ë¶€ë™ì‚°", "ê¸ˆìœµ", "ê³„ì•½ì„œ"])

# ìœ íŠœë¸Œ ì˜ìƒ í¬ë¡¤ë§
def get_youtube_video_info(query):
    headers = {"User-Agent": "Mozilla/5.0"}
    search_query = urllib.parse.quote(query)
    url = f"https://www.youtube.com/results?search_query={search_query}"
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    for script in soup.find_all("script"):
        if "var ytInitialData" in script.text:
            match = re.search(r'var ytInitialData = ({.*?});', script.string or "", re.DOTALL)
            if match:
                try:
                    data = json.loads(match.group(1))
                    items = data["contents"]["twoColumnSearchResultsRenderer"]["primaryContents"]["sectionListRenderer"]["contents"][0]["itemSectionRenderer"]["contents"]
                    for item in items:
                        if "videoRenderer" in item:
                            video = item["videoRenderer"]
                            title = video["title"]["runs"][0]["text"]
                            video_id = video["videoId"]
                            thumbnail = video["thumbnail"]["thumbnails"][-1]["url"]
                            return {
                                "videoId": video_id,
                                "title": title,
                                "thumbnail": thumbnail
                            }
                except Exception as e:
                    print("íŒŒì‹± ì˜¤ë¥˜:", e)
    return None

# ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def get_news_snippets(query):
    try:
        search_query = urllib.parse.quote(query)
        url = f"https://search.naver.com/search.naver?where=news&query={search_query}"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")
        items = soup.select(".news_wrap.api_ani_send")
        news = []
        for item in items[:5]:
            title_tag = item.select_one(".news_tit")
            summary_tag = item.select_one(".dsc_wrap")
            if title_tag and summary_tag:
                news.append({
                    "title": title_tag.get("title"),
                    "link": title_tag.get("href"),
                    "summary": summary_tag.get_text()
                })
        return news
    except Exception as e:
        return []

# ì„œìš¸ë²•ì› ê³„ì•½ì„œ ëª©ë¡ í¬ë¡¤ë§
def get_contract_list():
    url = "https://seoul.scourt.go.kr/contract/new/DocListAction.work"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, "html.parser")
    links = soup.select("a")
    contracts = []
    for link in links:
        href = link.get("href", "")
        text = link.get_text(strip=True)
        if "DocDownAction" in href and text:
            full_url = "https://seoul.scourt.go.kr" + href
            contracts.append({"name": text, "url": full_url})
    return contracts

# ì£¼ì œë³„ ì •ë³´ ì¶œë ¥
if main_topic:
    topic_data = get_topic_data(main_topic)
    if topic_data:
        sub_topic = st.radio(translate("ì„¸ë¶€ í•­ëª©ì„ ê³¨ë¼ë³´ì„¸ìš”"), list(topic_data.keys()))

        # âœ… íƒ­ êµ¬ì„±
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            translate("ğŸ“– ì •ë³´ ë³´ê¸°"),
            translate("ğŸ“° ê´€ë ¨ ë‰´ìŠ¤"),
            translate("ğŸ“„ ê³„ì•½ì„œ ì–‘ì‹"),
            translate("ğŸ¬ ìœ íŠœë¸Œ ì˜ìƒ"),
            translate("ğŸ’¬ ì˜ê²¬ ë‚¨ê¸°ê¸°")
        ])

        # ğŸ“– ì •ë³´ ë³´ê¸°
        with tab1:
            st.subheader(translate("ğŸ“– ì„ íƒí•œ ì •ë³´"))
            item = topic_data.get(sub_topic, {})
            if isinstance(item, dict):
                content = item.get("ë‚´ìš©", "ì •ë³´ ì—†ìŒ")
                source = item.get("ì¶œì²˜", "")
                st.success(translate(content))
                if source:
                    st.markdown(f"ğŸ”— ì¶œì²˜: [{source}]({source})")
            else:
                st.success(translate(item))

        # ğŸ“° ê´€ë ¨ ë‰´ìŠ¤
        with tab2:
            st.subheader(translate("ğŸ“° ê´€ë ¨ ë‰´ìŠ¤ ë³´ê¸°"))
            search_keyword = f"{main_topic} ê´€ë ¨ ë‰´ìŠ¤ {sub_topic}"
            news_items = get_news_snippets(search_keyword)
            if news_items:
                for news in news_items:
                    st.markdown(f"**[{news['title']}]({news['link']})**")
                    st.caption(news['summary'])
            else:
                st.warning(translate("ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))

        # ğŸ“„ ê³„ì•½ì„œ íƒ­
        with tab3:
            if main_topic == "ê³„ì•½ì„œ":
                st.subheader(translate("ğŸ“„ ì„œìš¸ë²•ì› ê³„ì•½ì„œ ì—´ëŒ ë° ë‹¤ìš´ë¡œë“œ"))
                contract_list = get_contract_list()
                if contract_list:
                    for contract in contract_list:
                        st.markdown(f"ğŸ“„ **{contract['name']}** ğŸ‘‰ [ë‹¤ìš´ë¡œë“œ]({contract['url']})")
                else:
                    st.warning(translate("ê³„ì•½ì„œ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
            else:
                st.info(translate("ê³„ì•½ì„œ ê´€ë ¨ í•­ëª©ì—ì„œë§Œ ì–‘ì‹ì´ ì œê³µë©ë‹ˆë‹¤."))

        # ğŸ¬ ìœ íŠœë¸Œ ì˜ìƒ
        with tab4:
            st.subheader(translate("ğŸ¬ ìœ íŠœë¸Œ ì˜ìƒ"))
            video = get_youtube_video_info(f"{main_topic} {sub_topic}")
            if video:
                st.image(video["thumbnail"], caption=translate(video["title"]))
                st.markdown(f"[ğŸ”— ì˜ìƒ ë³´ê¸°](https://www.youtube.com/watch?v={video['videoId']})")
            else:
                st.warning(translate("ìœ íŠœë¸Œ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))

        # ğŸ’¬ ì˜ê²¬ ë‚¨ê¸°ê¸°
        with tab5:
            st.subheader(translate("ğŸ’¬ ì˜ê²¬ ë‚¨ê¸°ê¸°"))
            feedback = st.text_area(translate("ê¶ê¸ˆí•œ ì ì´ë‚˜ ìš”ì²­í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ì ì–´ì£¼ì„¸ìš”"))
            if st.button(translate("ì œì¶œ")):
                st.success(translate("ì†Œì¤‘í•œ ì˜ê²¬ ê°ì‚¬í•©ë‹ˆë‹¤! ë¹ ë¥¸ ì‹œì¼ ë‚´ ë°˜ì˜í•˜ê² ìŠµë‹ˆë‹¤."))
    else:
        st.warning(translate("ì„ íƒí•œ ì£¼ì œì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."))

